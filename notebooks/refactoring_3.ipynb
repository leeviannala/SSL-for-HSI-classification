{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77869fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import spectral\n",
    "import imageio\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a70efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_new(x):\n",
    "    ret = np.zeros(x.shape)\n",
    "    filt = x == x\n",
    "    x_new = np.nan_to_num(x[np.where(filt)])\n",
    "    x_row_max = np.nanmax(x_new, axis=-1)\n",
    "    x_row_max = x_row_max.reshape(list(x_new.shape)[:-1] + [1])\n",
    "    x_new = x_new - x_row_max\n",
    "    x_exp = np.exp(x_new)\n",
    "    x_exp_row_sum = x_exp.sum(axis=-1).reshape(list(x.shape)[:-1] + [1])\n",
    "    softmax = x_exp / x_exp_row_sum\n",
    "    ret[np.where(filt)] = softmax\n",
    "    return ret\n",
    "def sample_gt3_new(img, gt, train_gt, test_gt, SAMPLE_PERCENTAGE, IGNORED_LABELS, ALL_LABELS):\n",
    "    X = img\n",
    "    Y = gt\n",
    "    labels = np.unique(Y)\n",
    "    labels = np.array([val for val in labels if not val in IGNORED_LABELS])\n",
    "    print(labels)\n",
    "    row, col, n_band = X.shape\n",
    "    num_class = len(ALL_LABELS) # TODO: Fix to the correct amount\n",
    "    max_class = np.max(ALL_LABELS)\n",
    "    # num_class = len(np.unique(labels))\n",
    "    first = True\n",
    "    skipped_labels = []\n",
    "    for i, val in enumerate(ALL_LABELS): # range(1, num_class + 1):\n",
    "        if val in labels:\n",
    "            index = np.where(train_gt == val)\n",
    "            index2 = np.where(test_gt == val)\n",
    "            if first:\n",
    "                array1_train = index[0]\n",
    "                array2_train = index[1]\n",
    "                array1_test = index2[0]\n",
    "                array2_test = index2[1]\n",
    "                first = False\n",
    "            else:\n",
    "                array1_train = np.concatenate((array1_train, index[0]))\n",
    "                array2_train = np.concatenate((array2_train, index[1]))\n",
    "                array1_test = np.concatenate((array1_test, index2[0]))\n",
    "                array2_test = np.concatenate((array2_test, index2[1]))\n",
    "        else:\n",
    "            skipped_labels.append(val)\n",
    "    y_train = Y[array1_train, array2_train]\n",
    "    trueEDtimesSID = []\n",
    "    trueEDtimesSID2 = []\n",
    "    sumtrueES = []\n",
    "    sumfalseES = []\n",
    "    pseudo_labels3 = np.zeros([row, col, num_class])\n",
    "    for i in range(0, len(array1_test)):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"i:%d\" % (i))\n",
    "        #if i%200!=0:\n",
    "        #    continue\n",
    "        xtest = array1_test[i]\n",
    "        ytest = array2_test[i]\n",
    "        labeltest = Y[xtest, ytest]\n",
    "        specvectortest = X[xtest, ytest]\n",
    "        # i\n",
    "        EDs = np.zeros(num_class)\n",
    "        SIDs = np.zeros(num_class)\n",
    "        EDtimesSIDs = np.zeros(num_class)\n",
    "        EDtimesSIDs2 = np.zeros(num_class)\n",
    "        minED = 10000000000\n",
    "        for j, val in enumerate(ALL_LABELS): #range(1, num_class + 1):  # 类别循环\n",
    "            if val in labels:\n",
    "                index2 = np.where(y_train == val)  ## 当前类别序号\n",
    "                index2 = index2[0]\n",
    "                EDsclass = []\n",
    "                SIDclass = []\n",
    "                EDtimesSIDclass = []\n",
    "                for nn in range(0, len(index2)):  # 类别内训练集循环 nn\n",
    "                    # print(index2[nn])##当前训练样本序号\n",
    "                    ind = index2[nn]\n",
    "                    xtrain = array1_train[ind]\n",
    "                    ytrain = array2_train[ind]\n",
    "                    specvectortrain = X[xtrain, ytrain]\n",
    "                    ED = np.sqrt(np.square(xtest - xtrain) + np.square(ytest - ytrain))\n",
    "                    SID1 = entropy(specvectortest, specvectortrain)\n",
    "                    SID2 = entropy(specvectortrain, specvectortest)\n",
    "                    SID = SID1 + SID2\n",
    "                    EDtimesSID = np.sqrt(ED * SID)\n",
    "                    ED = ED + SID\n",
    "                    EDsclass.append(ED)\n",
    "                    SIDclass.append(SID)\n",
    "                    EDtimesSIDclass.append(EDtimesSID)\n",
    "\n",
    "                    if ED < minED:\n",
    "                        minED = ED\n",
    "            # =================================\n",
    "                inde = np.argsort(EDsclass)\n",
    "\n",
    "                jiaquan = 0\n",
    "                for nn in range(0, len(index2)):\n",
    "                    jiaquandis = EDsclass[inde[nn]] * (float(num_class) ** (-nn))  # 类别内训练集循环 nn\n",
    "                    jiaquan = jiaquan + jiaquandis\n",
    "\n",
    "                EDs[j] = jiaquan\n",
    "                SIDs[j] = np.min(SIDclass)\n",
    "                EDtimesSIDs[j] = np.min(EDtimesSIDclass)\n",
    "                jiaquan2 = 0\n",
    "                inde2 = np.argsort(EDtimesSIDclass)\n",
    "                for nn in range(0, len(index2)):\n",
    "                    jiaquandis = EDtimesSIDclass[inde2[nn]] * (float(num_class) ** (-nn))  # 类别内训练集循环 nn\n",
    "                    jiaquan2 = jiaquan2 + jiaquandis\n",
    "                EDtimesSIDs2[j] = jiaquan2\n",
    "            else:\n",
    "                EDs[j] = np.nan\n",
    "                SIDs[j] = np.nan\n",
    "                EDtimesSIDs[j] = np.nan\n",
    "                EDtimesSIDs2[j] = np.nan\n",
    "        if np.nanmin(EDtimesSIDs) > 0.085:\n",
    "            continue\n",
    "        else:\n",
    "            minn = np.nanmin(EDs)\n",
    "            softm3 = softmax_new(-EDtimesSIDs2 * max_class*100)\n",
    "            minn\n",
    "        labeEDtimesSIDs = np.nanargmin(EDtimesSIDs)\n",
    "        labeEDtimesSIDs2 = np.nanargmin(EDtimesSIDs2)\n",
    "        train_gt[xtest, ytest] = labeEDtimesSIDs2\n",
    "        pseudo_labels3[xtest, ytest][ALL_LABELS] = softm3\n",
    "\n",
    "\n",
    "        if labeEDtimesSIDs == labeltest:\n",
    "            trueEDtimesSID.append(1)\n",
    "            sumtrueES.append(np.nanmin(EDtimesSIDs))\n",
    "        else:\n",
    "            trueEDtimesSID.append(0)\n",
    "            sumfalseES.append(np.nanmin(EDtimesSIDs))\n",
    "            print(\"falseEDtimesSID:\", np.nanmin(EDtimesSIDs))\n",
    "\n",
    "        if labeEDtimesSIDs2 == labeltest:\n",
    "            trueEDtimesSID2.append(1)\n",
    "        else:\n",
    "            trueEDtimesSID2.append(0)\n",
    "    accuEDtimesSID2 = np.sum(trueEDtimesSID2) / len(trueEDtimesSID2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gt(gt, train_size, mode='random'):\n",
    "    \"\"\"Extract a fixed percentage of samples from an array of labels.\n",
    "\n",
    "    Args:\n",
    "        gt: a 2D array of int labels\n",
    "        percentage: [0, 1] float\n",
    "    Returns:\n",
    "        train_gt, test_gt: 2D arrays of int labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    indices = np.nonzero(gt)\n",
    "    X = list(zip(*indices)) # x,y features\n",
    "    y = gt[indices].ravel() # classes\n",
    "    train_gt = np.zeros_like(gt)\n",
    "    test_gt = np.zeros_like(gt)\n",
    "    if train_size > 1:\n",
    "       train_size = int(train_size)\n",
    "    \n",
    "    if mode == 'random':\n",
    "       train_indices, test_indices = sklearn.model_selection.train_test_split(X, train_size=train_size, stratify=y)\n",
    "       train_indices = [list(t) for t in zip(*train_indices)]\n",
    "       test_indices = [list(t) for t in zip(*test_indices)]\n",
    "       train_gt[train_indices] = gt[train_indices]\n",
    "       test_gt[test_indices] = gt[test_indices]\n",
    "    elif mode == 'fixed':\n",
    "       # print(\"Sampling {} with train size = {}\".format(mode, train_size))\n",
    "       train_indices, test_indices = [], []\n",
    "        for c in np.unique(gt):\n",
    "            if c == 0:\n",
    "                continue\n",
    "           indices = np.nonzero(gt == c)\n",
    "           X = list(zip(*indices)) # x,y features\n",
    "           ##================\n",
    "           classsize = len(X)\n",
    "           train_size2=train_size\n",
    "           if classsize<=train_size:\n",
    "               train_size2=classsize-1\n",
    "           ##====================\n",
    "           train, test = sklearn.model_selection.train_test_split(X, train_size=train_size2)\n",
    "           train_indices += train\n",
    "           test_indices += test\n",
    "       train_indices = [list(t) for t in zip(*train_indices)]\n",
    "       test_indices = [list(t) for t in zip(*test_indices)]\n",
    "       train_gt[train_indices] = gt[train_indices]\n",
    "       test_gt[test_indices] = gt[test_indices]\n",
    "\n",
    "    elif mode == 'disjoint':\n",
    "        train_gt = np.copy(gt)\n",
    "        test_gt = np.copy(gt)\n",
    "        for c in np.unique(gt):\n",
    "            mask = gt == c\n",
    "            for x in range(gt.shape[0]):\n",
    "                first_half_count = np.count_nonzero(mask[:x, :])\n",
    "                second_half_count = np.count_nonzero(mask[x:, :])\n",
    "                try:\n",
    "                    ratio = first_half_count / second_half_count\n",
    "                    if ratio > 0.9 * train_size and ratio < 1.1 * train_size:\n",
    "                        break\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "            mask[:x, :] = 0\n",
    "            train_gt[mask] = 0\n",
    "\n",
    "        test_gt[train_gt > 0] = 0\n",
    "    else:\n",
    "        raise ValueError(\"{} sampling is not implemented yet.\".format(mode))\n",
    "    return train_gt, test_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6537967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(dataset): #TODO: implement opening dataset containing multiple files for tif files\n",
    "    _, ext = os.path.splitext(dataset)\n",
    "    ext = ext.lower()\n",
    "    if ext == '.mat':\n",
    "        # Load Matlab array\n",
    "        return io.loadmat(dataset)\n",
    "    elif ext == '.tif' or ext == '.tiff':\n",
    "        # Load TIFF file\n",
    "        return imageio.imread(dataset)\n",
    "    elif ext == '.hdr':\n",
    "        img = spectral.open_image(dataset)\n",
    "        return img.load()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown file format: {}\".format(ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daa692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_CONFIG = {\n",
    "        'PaviaC': {\n",
    "            'urls': ['http://www.ehu.eus/ccwintco/uploads/e/e3/Pavia.mat', \n",
    "                     'http://www.ehu.eus/ccwintco/uploads/5/53/Pavia_gt.mat'],\n",
    "            'img': 'Pavia.mat',\n",
    "            'gt': 'Pavia_gt.mat'\n",
    "            },\n",
    "        'PaviaU': {\n",
    "            'urls': ['http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat',\n",
    "                     'http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat'],\n",
    "            'img': 'PaviaU.mat',\n",
    "            'gt': 'PaviaU_gt.mat'\n",
    "            },\n",
    "        'KSC': {\n",
    "            'urls': ['http://www.ehu.es/ccwintco/uploads/2/26/KSC.mat',\n",
    "                     'http://www.ehu.es/ccwintco/uploads/a/a6/KSC_gt.mat'],\n",
    "            'img': 'KSC.mat',\n",
    "            'gt': 'KSC_gt.mat'\n",
    "            },\n",
    "        'IndianPines': {\n",
    "            'urls': ['http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat',\n",
    "                     'http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat'],\n",
    "            'img': 'Indian_pines_corrected.mat',\n",
    "            'gt': 'Indian_pines_gt.mat'\n",
    "            },\n",
    "        'Botswana': {\n",
    "            'urls': ['http://www.ehu.es/ccwintco/uploads/7/72/Botswana.mat',\n",
    "                     'http://www.ehu.es/ccwintco/uploads/5/58/Botswana_gt.mat'],\n",
    "            'img': 'Botswana.mat',\n",
    "            'gt': 'Botswana_gt.mat',\n",
    "            },\n",
    "        'Houston': {\n",
    "            'urls': ['http://www.ehu.es/ccwintco/uploads/7/72/Botswana.mat',\n",
    "                 'http://www.ehu.es/ccwintco/uploads/5/58/Botswana_gt.mat'],\n",
    "            'img': 'Houston.mat',\n",
    "            'gt': 'Houston_gt.mat',\n",
    "            },\n",
    "        'hyrank': {\n",
    "            'urls': ['', ''],\n",
    "            'img': 'Training Set/Anafi.tif',\n",
    "            'gt': 'Training Set/Anafi_GT.tif'\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0c90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name, target_folder=\"./\", datasets=DATASETS_CONFIG):\n",
    "    # TODO: Add Hyrank\n",
    "    \"\"\" Gets the dataset specified by name and return the related components.\n",
    "    Args:\n",
    "        dataset_name: string with the name of the dataset\n",
    "        target_folder (optional): folder to store the datasets, defaults to ./\n",
    "        datasets (optional): dataset configuration dictionary, defaults to prebuilt one\n",
    "    Returns:\n",
    "        img: 3D hyperspectral image (WxHxB)\n",
    "        gt: 2D int array of labels\n",
    "        label_values: list of class names\n",
    "        ignored_labels: list of int classes to ignore\n",
    "        rgb_bands: int tuple that correspond to red, green and blue bands\n",
    "    \"\"\"\n",
    "    palette = None\n",
    "    \n",
    "    if dataset_name not in list(datasets.keys()):\n",
    "        raise ValueError(\"{} dataset is unknown.\".format(dataset_name))\n",
    "\n",
    "    dataset = datasets[dataset_name]\n",
    "\n",
    "    folder = target_folder + datasets[dataset_name].get('folder', dataset_name + '/')\n",
    "    if dataset.get('download', True):\n",
    "        # Download the dataset if is not present\n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)\n",
    "        for url in datasets[dataset_name]['urls']:\n",
    "            # download the files\n",
    "            filename = url.split('/')[-1]\n",
    "            if not os.path.exists(folder + filename):\n",
    "                with TqdmUpTo(unit='B', unit_scale=True, miniters=1,\n",
    "                          desc=\"Downloading {}\".format(filename)) as t:\n",
    "                    urlretrieve(url, filename=folder + filename,\n",
    "                                     reporthook=t.update_to)\n",
    "    elif not os.path.isdir(folder):\n",
    "       print(\"WARNING: {} is not downloadable.\".format(dataset_name))\n",
    "\n",
    "    if dataset_name == 'PaviaC':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'Pavia.mat')['pavia']\n",
    "\n",
    "        rgb_bands = (55, 41, 12)\n",
    "\n",
    "        gt = open_file(folder + 'Pavia_gt.mat')['pavia_gt']\n",
    "\n",
    "        label_values = [\"Undefined\", \"Water\", \"Trees\", \"Asphalt\",\n",
    "                        \"Self-Blocking Bricks\", \"Bitumen\", \"Tiles\", \"Shadows\",\n",
    "                        \"Meadows\", \"Bare Soil\"]\n",
    "        all_labels = np.arange(len(label_values))\n",
    "\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "\n",
    "    elif dataset_name == 'PaviaU':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'PaviaU.mat')['paviaU']\n",
    "\n",
    "        rgb_bands = (55, 41, 12)\n",
    "\n",
    "        gt = open_file(folder + 'PaviaU_gt.mat')['paviaU_gt']\n",
    "\n",
    "        label_values = ['Undefined', 'Asphalt', 'Meadows', 'Gravel', 'Trees',\n",
    "                        'Painted metal sheets', 'Bare Soil', 'Bitumen',\n",
    "                        'Self-Blocking Bricks', 'Shadows']\n",
    "        all_labels = np.arange(len(label_values))\n",
    "\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "\n",
    "    elif dataset_name == 'IndianPines':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'Indian_pines_corrected.mat')\n",
    "        img = img['indian_pines_corrected']\n",
    "\n",
    "        rgb_bands = (43, 21, 11)  # AVIRIS sensor\n",
    "\n",
    "        gt = open_file(folder + 'Indian_pines_gt.mat')['indian_pines_gt']\n",
    "        label_values = [\"Undefined\", \"Alfalfa\", \"Corn-notill\", \"Corn-mintill\",\n",
    "                        \"Corn\", \"Grass-pasture\", \"Grass-trees\",\n",
    "                        \"Grass-pasture-mowed\", \"Hay-windrowed\", \"Oats\",\n",
    "                        \"Soybean-notill\", \"Soybean-mintill\", \"Soybean-clean\",\n",
    "                        \"Wheat\", \"Woods\", \"Buildings-Grass-Trees-Drives\",\n",
    "                        \"Stone-Steel-Towers\"]\n",
    "        all_labels = np.arange(len(label_values))\n",
    "\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'Houston':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'Houston.mat')\n",
    "        img = img['img']\n",
    "\n",
    "        rgb_bands = (43, 21, 11)  # AVIRIS sensor\n",
    "\n",
    "        gt = open_file(folder + 'Houston_gt.mat')['Houston_gt']\n",
    "        label_values = [\"Undefined\", \"1\", \"Corn-2\", \"Corn-3\",\n",
    "                        \"4\", \"Grass-5\", \"6-trees\",\n",
    "                        \"7-pasture-mowed\", \"Hay-8\", \"9\",\n",
    "                        \"Soybean-10\", \"11-mintill\", \"12-clean\",\n",
    "                        \"13\", \"14\", \"15-Grass-Trees-Drives\"]\n",
    "        all_labels = np.arange(len(label_values))\n",
    "\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'Botswana':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'Botswana.mat')['Botswana']\n",
    "\n",
    "        rgb_bands = (75, 33, 15)\n",
    "\n",
    "        gt = open_file(folder + 'Botswana_gt.mat')['Botswana_gt']\n",
    "        label_values = [\"Undefined\", \"Water\", \"Hippo grass\",\n",
    "                        \"Floodplain grasses 1\", \"Floodplain grasses 2\",\n",
    "                        \"Reeds\", \"Riparian\", \"Firescar\", \"Island interior\",\n",
    "                        \"Acacia woodlands\", \"Acacia shrublands\",\n",
    "                        \"Acacia grasslands\", \"Short mopane\", \"Mixed mopane\",\n",
    "                        \"Exposed soils\"]\n",
    "\n",
    "        all_labels = np.arange(len(label_values))\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'KSC':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'KSC.mat')['KSC']\n",
    "\n",
    "        rgb_bands = (43, 21, 11)  # AVIRIS sensor\n",
    "\n",
    "        gt = open_file(folder + 'KSC_gt.mat')['KSC_gt']\n",
    "        label_values = [\"Undefined\", \"Scrub\", \"Willow swamp\",\n",
    "                        \"Cabbage palm hammock\", \"Cabbage palm/oak hammock\",\n",
    "                        \"Slash pine\", \"Oak/broadleaf hammock\",\n",
    "                        \"Hardwood swamp\", \"Graminoid marsh\", \"Spartina marsh\",\n",
    "                        \"Cattail marsh\", \"Salt marsh\", \"Mud flats\", \"Wate\"]\n",
    "\n",
    "        all_labels = np.arange(len(label_values))\n",
    "        ignored_labels = [0]\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'hyrank':\n",
    "        files = ['Training Set/Anafi.tif',\n",
    "                 'Training Set/Atokos.tif',\n",
    "                 'Training Set/Donousa.tif',\n",
    "                 'Validation Set/Kasos.tif',\n",
    "                 'Validation Set/Tilos.tif',\n",
    "                ]\n",
    "        \n",
    "        files_gt = ['Training Set/Anafi_GT.tif',\n",
    "                    'Training Set/Atokos_GT.tif',\n",
    "                    'Training Set/Donousa_GT.tif',\n",
    "                    'Validation Set/Kasos_GT.tif',\n",
    "                    'Validation Set/Tilos_GT.tif',\n",
    "                    ]\n",
    "        img = [open_file(folder + i) for i in files]\n",
    "\n",
    "        rgb_bands = [30,20,2]\n",
    "\n",
    "        gt = [open_file(folder + i) for i in files_gt]\n",
    "\n",
    "        label_values = ['Undefined',\n",
    "                        'High intensity developed', \n",
    "                        'Med-low intensity developed',\n",
    "                        'Deciduous, Evergreen, mixed forest',\n",
    "                        'shrubland',\n",
    "                        'Grassland-Pasture',\n",
    "                        'Bareland',\n",
    "                        'water',\n",
    "                        'corn',\n",
    "                        'cotton',\n",
    "                        'cereals',\n",
    "                        'almonds',\n",
    "                        'grass fodders',\n",
    "                        'vinewards-grapes',\n",
    "                        'walnuts',\n",
    "                        'pistachios',\n",
    "                        'citrus',\n",
    "                        'fallow']\n",
    "\n",
    "        all_labels = np.arange(len(label_values))\n",
    "\n",
    "\n",
    "        ignored_labels = [0]\n",
    "    else:\n",
    "        # Custom dataset\n",
    "        img, gt, rgb_bands, ignored_labels, label_values, palette = CUSTOM_DATASETS_CONFIG[dataset_name]['loader'](folder)\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    # Filter NaN out\n",
    "    for i, im in enumerate(img):\n",
    "        nan_mask = np.isnan(im.sum(axis=-1))\n",
    "        if np.count_nonzero(nan_mask) > 0:\n",
    "            print(\"Warning: NaN have been found in the data. It is preferable to remove them beforehand. Learning on NaN data is disabled.\")\n",
    "            img[i][nan_mask] = 0\n",
    "            gt[i][nan_mask] = 0\n",
    "            ignored_labels.append(0)\n",
    "\n",
    "        # Normalization\n",
    "        im = np.asarray(im, dtype='float32')\n",
    "        #img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        data = im.reshape(np.prod(im.shape[:2]), np.prod(im.shape[2:]))\n",
    "        #data = preprocessing.scale(data)\n",
    "        data  = preprocessing.minmax_scale(data)\n",
    "        img[i] = data.reshape(im.shape)\n",
    "    ignored_labels = list(set(ignored_labels))\n",
    "    return img, gt, label_values, ignored_labels, all_labels, rgb_bands, palette\n",
    "\n",
    "\n",
    "def get_originate_dataset(dataset_name, target_folder=\"./\", datasets=DATASETS_CONFIG):\n",
    "\n",
    "\n",
    "    if dataset_name not in list(datasets.keys()):\n",
    "        raise ValueError(\"{} dataset is unknown.\".format(dataset_name))\n",
    "\n",
    "    dataset = datasets[dataset_name]\n",
    "\n",
    "    folder = target_folder + datasets[dataset_name].get('folder', dataset_name + '/')\n",
    "\n",
    "\n",
    "    if dataset_name == 'PaviaC':\n",
    "        img = open_file(folder + 'Pavia.mat')['pavia']\n",
    "        gt = open_file(folder + 'Pavia_gt.mat')['pavia_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'PaviaU':\n",
    "        img = open_file(folder + 'PaviaU.mat')['paviaU']\n",
    "        gt = open_file(folder + 'PaviaU_gt.mat')['paviaU_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'IndianPines':\n",
    "        img = open_file(folder + 'Indian_pines_corrected.mat')\n",
    "        img = img['indian_pines_corrected']\n",
    "        rgb_bands = (43, 21, 11)  # AVIRIS sensor\n",
    "        gt = open_file(folder + 'Indian_pines_gt.mat')['indian_pines_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'Botswana':\n",
    "        img = open_file(folder + 'Botswana.mat')['Botswana']\n",
    "        gt = open_file(folder + 'Botswana_gt.mat')['Botswana_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'KSC':\n",
    "        img = open_file(folder + 'KSC.mat')['KSC']\n",
    "        gt = open_file(folder + 'KSC_gt.mat')['KSC_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'Houston':\n",
    "        img = open_file(folder + 'Houston.mat')\n",
    "        img = img['img']\n",
    "        gt = open_file(folder + 'Houston_gt.mat')['Houston_gt']\n",
    "        img = [img]\n",
    "        gt = [gt]\n",
    "    elif dataset_name == 'hyrank':\n",
    "        files = ['Training Set/Anafi.tif',\n",
    "                 'Training Set/Atokos.tif',\n",
    "                 'Training Set/Donousa.tif',\n",
    "                 'Validation Set/Kasos.tif',\n",
    "                 'Validation Set/Tilos.tif',\n",
    "                ]\n",
    "        \n",
    "        files_gt = ['Training Set/Anafi_GT.tif',\n",
    "                    'Training Set/Atokos_GT.tif',\n",
    "                    'Training Set/Donousa_GT.tif',\n",
    "                    'Validation Set/Kasos_GT.tif',\n",
    "                    'Validation Set/Tilos_GT.tif',\n",
    "                    ]\n",
    "        img = [open_file(folder + i) for i in files]\n",
    "        gt = [open_file(folder + i) for i in files_gt]\n",
    "    return img, gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99bbb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(DATASET, datasets_root, SAMPLE_PERCENTAGE):\n",
    "    img, gt, LABEL_VALUES, IGNORED_LABELS, ALL_LABELS, _, _ = get_dataset(DATASET, datasets_root)\n",
    "    X, Y = get_originate_dataset(DATASET, datasets_root)\n",
    "    img = concat_with_padding(img, [1])\n",
    "    img = img[:, :, list(range(0, 102, 3))] # Why is this here? This takes the every third channel from every dataset.\n",
    "                                            # The end point being fixed at 102 might be a problem. \n",
    "                                            # On the other hand no similar thing is done to X, which is supposedly\n",
    "                                            # the same image. \n",
    "\n",
    "    N_CLASSES = len(LABEL_VALUES)\n",
    "    INPUT_SIZE = np.shape(img)[-1]\n",
    "    # train_gt, test_gt = sample_gt(gt, SAMPLE_PERCENTAGE, mode='fixed')\n",
    "    train_test_gt = [sample_gt(i, SAMPLE_PERCENTAGE, mode='fixed') for i in gt]\n",
    "    #train_gt = sample_gt2(X, Y, train_gt, test_gt, SAMPLE_PERCENTAGE)\n",
    "    # # # breakpoint()\n",
    "    pseudo_labelpath= '../' + str(DATASET) + f'/pseudo_labels/pseudo_labels3/pseudo_labels3_{SAMPLE_PERCENTAGE}.npy'\n",
    "    pseudo_labels3 = []\n",
    "    if not os.path.exists(pseudo_labelpath):\n",
    "        newdir = str(DATASET) + '/pseudo_labels/pseudo_labels3/'\n",
    "        if not os.path.exists(newdir):\n",
    "            os.makedirs(newdir)\n",
    "        for x, y, tr_te_gt in zip(X, Y, train_test_gt):\n",
    "            pseudo_labels3.append(sample_gt3_new(x, y, tr_te_gt[0], tr_te_gt[1], \n",
    "                                                 SAMPLE_PERCENTAGE, IGNORED_LABELS, ALL_LABELS))\n",
    "        pseudo_labels3 = concat_with_padding(pseudo_labels3, [1])\n",
    "        np.save(pseudo_labelpath, pseudo_labels3)\n",
    "    else:\n",
    "        pseudo_labels3=np.load(pseudo_labelpath)\n",
    "    train_gt = concat_with_padding([i[0] for i in train_test_gt], [1])\n",
    "    test_gt = concat_with_padding([i[1] for i in train_test_gt], [1])\n",
    "    gt = concat_with_padding(gt, [1]) \n",
    "    X = concat_with_padding(X, [1])\n",
    "    Y = concat_with_padding(Y, [1])\n",
    "    return img, gt, LABEL_VALUES, IGNORED_LABELS, ALL_LABELS, X, Y, N_CLASSES,\\\n",
    "           INPUT_SIZE, train_gt, test_gt, pseudo_labels3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e00abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pad(pad_value):\n",
    "        if pad_value%2 == 0:\n",
    "            return [int(pad_value/2), int(pad_value/2)]\n",
    "        else:\n",
    "            return [int(pad_value/2), int(pad_value/2) + 1]\n",
    "def calc_pad(mx, shape, axes):\n",
    "    pad_values = mx - shape\n",
    "    pads = [[0,0], [0,0], [0,0]]\n",
    "    for axis in axes:\n",
    "        #print(i)\n",
    "        #print(i in axes)\n",
    "        pads[axis] = (single_pad(pad_values[axis]))\n",
    "    # print(pads)\n",
    "    return pads\n",
    "    \n",
    "def concat_with_padding(list_of_arrays, padding_axes):\n",
    "    shape1s = [i.shape for i in list_of_arrays]\n",
    "    max_shape = np.max(shape1s, axis=0)\n",
    "    #max_shape = np.array(img[-1].shape)\n",
    "    pads = [calc_pad(max_shape, i, padding_axes) for i in shape1s]\n",
    "    img2 = [np.pad(list_of_arrays[i], pad) for i, pad in enumerate(pads)]\n",
    "    res2 = np.concatenate(img2)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87059de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1024889/2054537087.py:9: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  return imageio.imread(dataset)\n",
      "/tmp/ipykernel_1024889/2054537087.py:9: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  return imageio.imread(dataset)\n",
      "/tmp/ipykernel_1024889/1608058974.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  train_gt[train_indices] = gt[train_indices]\n",
      "/tmp/ipykernel_1024889/1608058974.py:46: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  test_gt[test_indices] = gt[test_indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17]\n",
      "i:0\n",
      "i:1000\n",
      "i:2000\n",
      "i:3000\n",
      "i:4000\n",
      "i:5000\n",
      "i:6000\n",
      "i:7000\n",
      "i:8000\n",
      "i:9000\n",
      "i:10000\n",
      "i:11000\n",
      "i:12000\n",
      "i:13000\n",
      "i:14000\n",
      "i:15000\n",
      "i:16000\n",
      "i:17000\n",
      "i:18000\n",
      "i:19000\n",
      "i:20000\n",
      "i:21000\n",
      "i:22000\n",
      "i:23000\n",
      "i:24000\n",
      "i:25000\n",
      "i:26000\n",
      "i:27000\n",
      "i:28000\n",
      "i:29000\n",
      "i:30000\n",
      "i:31000\n",
      "i:32000\n",
      "i:33000\n",
      "i:34000\n",
      "i:35000\n",
      "i:36000\n",
      "i:37000\n",
      "i:38000\n",
      "i:39000\n",
      "i:40000\n",
      "i:41000\n",
      "i:42000\n",
      "i:43000\n",
      "i:44000\n",
      "i:45000\n",
      "i:46000\n",
      "i:47000\n",
      "i:48000\n",
      "i:49000\n",
      "i:50000\n",
      "i:51000\n",
      "i:52000\n",
      "i:53000\n",
      "i:54000\n",
      "i:55000\n",
      "i:56000\n",
      "i:57000\n",
      "i:58000\n",
      "i:59000\n",
      "i:60000\n",
      "i:61000\n",
      "i:62000\n",
      "i:63000\n",
      "i:64000\n",
      "i:65000\n",
      "i:66000\n",
      "i:67000\n",
      "i:68000\n",
      "i:69000\n",
      "i:70000\n",
      "i:71000\n",
      "i:72000\n",
      "i:73000\n",
      "i:74000\n",
      "i:75000\n",
      "i:76000\n",
      "i:77000\n",
      "i:78000\n",
      "i:79000\n",
      "i:80000\n",
      "i:81000\n",
      "i:82000\n",
      "i:83000\n",
      "i:84000\n",
      "i:85000\n",
      "i:86000\n",
      "i:87000\n",
      "i:88000\n",
      "i:89000\n",
      "i:90000\n",
      "i:91000\n",
      "i:92000\n",
      "i:93000\n",
      "i:94000\n",
      "i:95000\n",
      "i:96000\n",
      "i:97000\n",
      "i:98000\n",
      "i:99000\n",
      "i:100000\n",
      "i:101000\n",
      "i:102000\n",
      "i:103000\n",
      "i:104000\n",
      "i:105000\n",
      "i:106000\n",
      "i:107000\n",
      "i:108000\n",
      "i:109000\n",
      "i:110000\n",
      "i:111000\n",
      "i:112000\n",
      "i:113000\n",
      "i:114000\n",
      "i:115000\n",
      "i:116000\n",
      "i:117000\n",
      "i:118000\n",
      "i:119000\n",
      "i:120000\n",
      "i:121000\n",
      "i:122000\n",
      "i:123000\n",
      "i:124000\n",
      "i:125000\n",
      "i:126000\n",
      "i:127000\n",
      "i:128000\n",
      "i:129000\n",
      "i:130000\n",
      "i:131000\n",
      "i:132000\n",
      "i:133000\n",
      "i:134000\n",
      "i:135000\n",
      "i:136000\n",
      "i:137000\n",
      "i:138000\n",
      "i:139000\n",
      "i:140000\n",
      "i:141000\n",
      "i:142000\n",
      "i:143000\n",
      "i:144000\n",
      "i:145000\n",
      "i:146000\n",
      "i:147000\n",
      "i:148000\n",
      "i:149000\n",
      "i:150000\n",
      "i:151000\n",
      "i:152000\n",
      "i:153000\n",
      "i:154000\n",
      "i:155000\n",
      "i:156000\n",
      "i:157000\n",
      "i:158000\n",
      "i:159000\n",
      "i:160000\n",
      "i:161000\n",
      "i:162000\n",
      "i:163000\n",
      "i:164000\n",
      "i:165000\n",
      "i:166000\n",
      "i:167000\n",
      "i:168000\n",
      "i:169000\n",
      "i:170000\n",
      "i:171000\n",
      "i:172000\n",
      "i:173000\n",
      "i:174000\n",
      "i:175000\n",
      "i:176000\n",
      "i:177000\n",
      "i:178000\n",
      "i:179000\n",
      "i:180000\n",
      "i:181000\n",
      "i:182000\n",
      "i:183000\n",
      "i:184000\n",
      "i:185000\n",
      "i:186000\n",
      "i:187000\n",
      "i:188000\n",
      "i:189000\n",
      "i:190000\n",
      "i:191000\n",
      "i:192000\n",
      "i:193000\n",
      "i:194000\n",
      "i:195000\n",
      "i:196000\n",
      "i:197000\n",
      "i:198000\n",
      "i:199000\n",
      "i:200000\n",
      "i:201000\n",
      "i:202000\n",
      "i:203000\n",
      "i:204000\n",
      "i:205000\n",
      "i:206000\n",
      "i:207000\n",
      "i:208000\n",
      "i:209000\n",
      "i:210000\n",
      "i:211000\n",
      "i:212000\n",
      "i:213000\n",
      "i:214000\n",
      "i:215000\n",
      "i:216000\n",
      "i:217000\n",
      "i:218000\n",
      "i:219000\n",
      "i:220000\n",
      "i:221000\n",
      "i:222000\n",
      "i:223000\n",
      "i:224000\n",
      "i:225000\n",
      "i:226000\n",
      "i:227000\n",
      "i:228000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m img, gt, LABEL_VALUES, IGNORED_LABELS, ALL_LABELS,\\\n\u001b[0;32m----> 2\u001b[0m X, Y, N_CLASSES, INPUT_SIZE, train_gt, test_gt, pseudo_labels3 \u001b[38;5;241m=\u001b[39m \u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyrank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/data/leevi/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mload_datasets\u001b[0;34m(DATASET, datasets_root, SAMPLE_PERCENTAGE)\u001b[0m\n\u001b[1;32m     21\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(newdir)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, tr_te_gt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y, train_test_gt):\n\u001b[0;32m---> 23\u001b[0m     pseudo_labels3\u001b[38;5;241m.\u001b[39mappend(\u001b[43msample_gt3_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_te_gt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_te_gt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mSAMPLE_PERCENTAGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIGNORED_LABELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALL_LABELS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m pseudo_labels3 \u001b[38;5;241m=\u001b[39m concat_with_padding(pseudo_labels3, [\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m np\u001b[38;5;241m.\u001b[39msave(pseudo_labelpath, pseudo_labels3)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36msample_gt3_new\u001b[0;34m(img, gt, train_gt, test_gt, SAMPLE_PERCENTAGE, IGNORED_LABELS, ALL_LABELS)\u001b[0m\n\u001b[1;32m     73\u001b[0m xtrain \u001b[38;5;241m=\u001b[39m array1_train[ind]\n\u001b[1;32m     74\u001b[0m ytrain \u001b[38;5;241m=\u001b[39m array2_train[ind]\n\u001b[0;32m---> 75\u001b[0m specvectortrain \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m ED \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msquare(xtest \u001b[38;5;241m-\u001b[39m xtrain) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(ytest \u001b[38;5;241m-\u001b[39m ytrain))\n\u001b[1;32m     77\u001b[0m SID1 \u001b[38;5;241m=\u001b[39m entropy(specvectortest, specvectortrain)\n",
      "File \u001b[0;32m/mnt/condaenvs/leevi/envs/hyspec/lib/python3.10/site-packages/imageio/core/util.py:149\u001b[0m, in \u001b[0;36mArray.__array_finalize__\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124;03m\"\"\"The dict with the meta data of this image.\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_finalize__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ob):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124;03m\"\"\"So the meta info is maintained when doing calculations with\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    the array.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ob, Array):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img, gt, LABEL_VALUES, IGNORED_LABELS, ALL_LABELS,\\\n",
    "X, Y, N_CLASSES, INPUT_SIZE, train_gt, test_gt, pseudo_labels3 = load_datasets('hyrank', '/mnt/data/leevi/', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4b9b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1024889/2054537087.py:9: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  return imageio.imread(dataset)\n"
     ]
    }
   ],
   "source": [
    "folder = '/mnt/data/leevi/hyrank/'\n",
    "files = ['Training Set/Anafi.tif',\n",
    "                 'Training Set/Atokos.tif',\n",
    "                 'Training Set/Donousa.tif',\n",
    "                 'Validation Set/Kasos.tif',\n",
    "                 'Validation Set/Tilos.tif',\n",
    "                ]\n",
    "        \n",
    "files_gt = ['Training Set/Anafi_GT.tif',\n",
    "            'Training Set/Atokos_GT.tif',\n",
    "            'Training Set/Donousa_GT.tif',\n",
    "            'Validation Set/Kasos_GT.tif',\n",
    "            'Validation Set/Tilos_GT.tif',\n",
    "            ]\n",
    "img = [open_file(folder + i) for i in files]\n",
    "gt = [open_file(folder + i) for i in files_gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad85ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16640, 710, 147)\n",
      "(16640, 710)\n",
      "-------\n",
      "(12000, 700, 147)\n",
      "(12000, 700)\n",
      "-------\n",
      "(10800, 660, 147)\n",
      "(10800, 660)\n",
      "-------\n",
      "(11620, 680, 147)\n",
      "(11620, 680)\n",
      "-------\n",
      "(14700, 740, 147)\n",
      "(14700, 740)\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(img[i].shape)\n",
    "    print(gt[i].shape)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e01ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "max_shape1 = 0\n",
    "for i, im in enumerate(img):\n",
    "    max_updated = False\n",
    "    max_equal = False\n",
    "    if max_shape1 == im.shape[1]:\n",
    "        max_equal = True\n",
    "    if max_shape1 < im.shape[1]:\n",
    "        max_updated = True\n",
    "        max_shape1 = max(im.shape[1], max_shape1)\n",
    "    if i == 0:\n",
    "        res = im\n",
    "    elif max_equal:\n",
    "        res = np.concatenate(res, im)\n",
    "    elif max_updated:\n",
    "        pad_value = max_shape1 - res.shape[1]\n",
    "        if pad_value%2 == 0:\n",
    "            pads = [[0,0], [int(pad_value/2), int(pad_value/2)], [0,0]]\n",
    "        else:\n",
    "            pads = [[0,0], [int(pad_value/2), int(pad_value/2) + 1], [0,0]]\n",
    "        res = np.concatenate([np.pad(res, pads), im])\n",
    "    else:\n",
    "        pad_value = max_shape1 - im.shape[1]\n",
    "        if pad_value%2 == 0:\n",
    "            pads = [[0,0], [int(pad_value/2), int(pad_value/2)], [0,0]]\n",
    "        else:\n",
    "            pads = [[0,0], [int(pad_value/2), int(pad_value/2) + 1], [0,0]]\n",
    "        res = np.concatenate([res, np.pad(im, pads)])\n",
    "res.shape\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e00950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.823099374771118\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "shape1s = [i.shape for i in img]\n",
    "max_shape = np.max(shape1s, axis=0)\n",
    "#max_shape = np.array(img[-1].shape)\n",
    "def single_pad(pad_value):\n",
    "    if pad_value%2 == 0:\n",
    "        return [int(pad_value/2), int(pad_value/2)]\n",
    "    else:\n",
    "        return [int(pad_value/2), int(pad_value/2) + 1]\n",
    "def calc_pad(mx, shape, axes):\n",
    "    pad_values = mx - shape\n",
    "    pads = [[0,0], [0,0], [0,0]]\n",
    "    for axis in axes:\n",
    "        #print(i)\n",
    "        #print(i in axes)\n",
    "        pads[axis] = (single_pad(pad_values[axis]))\n",
    "    # print(pads)\n",
    "    return pads\n",
    "pads = [calc_pad(max_shape, i, [1]) for i in shape1s]\n",
    "pads_gt = [[pad[0], pad[1]] for pad in pads]\n",
    "img2 = [np.pad(img[i], pad) for i, pad in enumerate(pads)]\n",
    "gts2 = [np.pad(gt[i], pad) for i, pad in enumerate(pads_gt)]\n",
    "res2 = np.concatenate(img2)\n",
    "gt2 = np.concatenate(gts2)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd740a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65760, 740)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a411f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape1s = [i.shape for i in img]\n",
    "max_shape = np.max(shape1s, axis=0)\n",
    "max_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.ones((15,10,5)), np.pad(np.ones([14, 5, 5]), calc_pad(10,5))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67901b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49549f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48470",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7194d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944bdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
